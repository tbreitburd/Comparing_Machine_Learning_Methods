\documentclass[12pt]{report} % Increased the font size to 12pt
\usepackage{epigraph}
\usepackage{geometry}

% Optional: customize the style of epigraphs
\setlength{\epigraphwidth}{0.5\textwidth} % Adjust the width of the epigraph
\renewcommand{\epigraphflush}{flushright} % Align the epigraph to the right
\renewcommand{\epigraphrule}{0pt} % No horizontal rule
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage{hyperref} % Added for hyperlinks
\usepackage{listings} % Added for code listings
\usepackage{color}    % Added for color definitions
\usepackage[super]{nth} 
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{cite}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Define the graphics path
%\graphicspath{{./Plots/}}

% Define the header and footer for general pages
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead{} % Initially, the header is empty
\fancyfoot[C]{\thepage} % Page number at the center of the footer
\renewcommand{\headrulewidth}{0pt} % No header line on the first page of chapters
\renewcommand{\footrulewidth}{0pt} % No footer line

% Define the plain page style for chapter starting pages
\fancypagestyle{plain}{%
  \fancyhf{} % Clear all header and footer fields
  \fancyfoot[C]{\thepage} % Page number at the center of the footer
  \renewcommand{\headrulewidth}{0pt} % No header line
}

% Apply the 'fancy' style to subsequent pages in a chapter
\renewcommand{\chaptermark}[1]{%
  \markboth{\MakeUppercase{#1}}{}%
}

% Redefine the 'plain' style for the first page of chapters
\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyfoot[C]{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

% Header settings for normal pages (not the first page of a chapter)
\fancyhead[L]{\slshape \nouppercase{\leftmark}} % Chapter title in the header
\renewcommand{\headrulewidth}{0.4pt} % Header line width on normal pages

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}
% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% Definition of the tcolorbox for definitions
\newtcolorbox{definitionbox}{
  colback=red!5!white,
  colframe=red!75!black,
  colbacktitle=red!85!black,
  title=Definition:,
  fonttitle=\bfseries,
  enhanced,
}

% Definition of the tcolorbox for remarks
\newtcolorbox{remarkbox}{
  colback=blue!5!white,     % Light blue background
  colframe=blue!75!black,   % Darker blue frame
  colbacktitle=blue!85!black, % Even darker blue for the title background
  title=Remark:,            % Title text for remark box
  fonttitle=\bfseries,      % Bold title font
  enhanced,
}

% Definition of the tcolorbox for examples
\newtcolorbox{examplebox}{
  colback=green!5!white,
  colframe=green!75!black,
  colbacktitle=green!85!black,
  title=Example:,
  fonttitle=\bfseries,
  enhanced,
}

% Definitions and examples will be put in these environments
\newenvironment{definition}
    {\begin{definitionbox}}
    {\end{definitionbox}}

\newenvironment{example}
    {\begin{examplebox}}
    {\end{examplebox}}

\geometry{top=1.5in} % Adjust the value as needed
% ----------------------------------------------------------------------------------------


\title{S1 Principles of Data Science Coursework Report}
\author{CRSiD: tmb76}
\date{University of Cambridge}

\begin{document} 

\maketitle

\chapter*{Section A}

\section*{Question 1}

The aim of this question is to explore the dataset and conduct some processing of the data. Conducting a check for missing values, which there were none, the data is explore in the following ways.

\subsection*{(a)}

The data first needs to be visualised in some way. Now because there are 500 features, and therefore 500 dimensions in the feature-space, this means that for now, only visualisations of each feature separately is useful. Thus a density plot of the first 20 features is obtained and shown below (Figure 1).  The main observation is that some features are very similar to each other, following similar distributions. A rough assumption is there are overall 3\-4 groups of highly correlated features for the entire dataset. The most common one has a very strong peak at 0, with a much smaller one at around 1. Another is bimodal, with peaks at 0 and 3. And the last one is also bimodal, with peaks at 0 and a stonger one at approximately 4. If the features can be grouped then one could reduce the dimensionality of the feature-space to 1 representative feature from each group.

\begin{figure}[hp]
    \centering
    \includegraphics[width=0.8\textwidth]{../Plots/A_Q1a.png}
    \caption{ \textbf{Density plots of the first 20 features, grouped by similarity}. The x-axis was set to the same scale for all plots, for easire comparison. The y-axis is however different for all plots.}
\end{figure}

\newpage

\subsection*{(b)}

Thus, Principle Component Analysis (PCA) is conducted on the dataset. PCA enables one to derive a lower-dimensional set of features from the full \textbf{A} dataset. This is done by finding the direction in which the observations have the greatest variance, for the full feature space\cite[pp. 255-257]{james2013introduction}. This is done on the entire dataset, not just the first 20 features. Using \texttt{scikit-learn}'s PCA function, getting the first 2 Principles Components (PC). The dimensionaly reduced dataset was obtained and the data was plotted in the following scatter plot (Figure 2).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q1b.png}
    \caption{\textbf{Scatter plot of the dataset after PCA.} Density contours are also shown.}
\end{figure}

From Figure 2, the bimodal separation seen in some of the first 20 features is seen again in the 1$^{st}$ PC. SMTHG about linear sep.\ The 2$^{nd}$ PC also resembles the more commonly observed feature in Figure 1, with a very strong peak and a much weaker one at a slightly higher value, more from the right-hand side group. This is hard to see in the scatter plot, but the density contours do show the negative skewness of the data's distribution for that PC.\

\subsection*{(c)}

From this, clustering can be run to try and see how they would compare to the distribution observed. The \texttt{scikit-learn} KMeans clustering algorithm was used, with all parameters set to their default values, apart from the random state.
The default number of clusters is 8. Now from Figure 1 and figure 2, it seems unlikely that there are 8 clusters in the data. This is one of the risks of K-means clustering as it will ``find'' 8 clusters, even if there are not 8 clusters in the data. To assess if ``default'' K-means clustering performs adequatly, the data is split into 2. For both these splits, clustering is run and then applied to the other split.  
The reason this can be done is that K-means clustering's main result is the position of points called centroids, which are the geometrical centres of each clusters (when defining the centroids from the feature means). In terms of their significance they are each the most representative points of each cluster. One of the \texttt{scikit-learn} K-means output is those \texttt{cluster\_centers\_}. Then when using the \texttt{.predict{}} method after fitting the model, i.e. clustering one half of the date, the other half can be clustered base on the distance of each point to the centroids of the clusters. This is done for both splits of the data and the results are shown in the tables below:

\begin{table}[h]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c| }
    \hline
    \textbf{Clusters} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{7} \\ 
    \hline
    \textbf{0} & 4 & 9 & 11 & 4 & 2 & 9 & 2 \\
    \hline
    \textbf{1} & 4 & 5 & 18 & 5 & 3 & 3 & 4 \\
    \hline
    \textbf{2} & 4 & 3 & 12 & 4 & 2 & 2 & 7 \\
    \hline
    \textbf{3} & 6 & 8 & 9 & 4 & 0 & 0 & 5 \\
    \hline
    \textbf{4} & 3 & 5 & 5 & 2 & 0 & 3 & 1 \\
    \hline
    \textbf{5} & 4 & 2 & 9 & 5 & 2 & 5 & 5 \\
    \hline
    \textbf{6} & 0 & 0 & 0 & 0 & 0 & 0 & 2 \\
    \hline
    \textbf{7} & 0 & 2 & 0 & 0 & 0 & 0 & 0 \\
    \hline

\end{tabular}
\caption{Contingency table for the 2 clusterings.}
\end{table}

\subsection*{(d)}

As can be seen, cluster 6 for the 2$^{nd}$ clustering (columns) is empty. This can be explained by the initialization of the centroids. Indeed the K-means clustering has to place the centroids somewhere at the start, and they are then updated based on which points are closest to it and their mean. If the centroid for cluster 6 in the 2$^{nd}$ clustering happened to have no points closer to it than other centroids, it stayed where it was for the whole clustering process\cite{scikit-learn}. Overall, the fact that a too large cluster number was selected for the model means there is almost no agreement between clusters (Table 1).  This means that the clusters are not very stable. Stability here refers to getting similar clusters when clustering the same or very similar datasets. Furthermore, the number used to designate the cluster is not the same in the 2 clusterings. In other words, it could be the case that the relatively greater agreement between cluster 1 from the 1$^{st}$ clustering (rows) and cluster 2 from the 2$^{nd}$ clustering (columns), is due to the fact they have centroids that are close to each other. But they happened to have different numbers. Regardless the agreement is small since all values in the table are small.  

Thus, we repeat the clustering for a lower number of clusters, 3, based on the visualisation of the data, Fig 2, and the actual knowledge of there being 3 clusters. We get the following results:


\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q1d_3clusters.png}
    \caption{\textbf{Scatter plot of the 2 clusterings} The colors indicate the cluster membership.}
\end{figure}

\subsection*{(e)}

Again, the number of the clusters are not the same but it is clear there is a much better agreement between the 2 clusterings. 

Conducting PCA before and after clustering is useful in both cases. One gives a clue as to how many clusters one should use in the K-means training and the other enables one to visualise the clustering results. One additional thing is that the K-means could have been done on the PCA reduced dataset, which would have been a 2D dataset. The impact on performance in that case is not completely clear.


\section*{Question 2}

The same dataset as in A is given. But here, the aim is to deal with duplicated observations and missing labels there are in this version of the dataset.

\subsection*{(a)}

First, the frequency of the labels is summarised in the table below (Table 2):

\begin{table}[h]
\centering
\begin{tabular}{ |c|c| }
    \hline
    \textbf{Label} & \textbf{Frequency} \\
    \hline
    \textbf{1.0} & 179 \\ 
    \hline
    \textbf{2.0} & 157 \\
    \hline
    \textbf{4.0} & 72 \\
    \hline
\end{tabular}
\caption{Frequency of the labels in the \textbf{B} dataset.}
\end{table}

\subsection*{(b)}

Taking only the feature columns, the following rows were identified as duplicates:

\begin{table}[h]
\centering
Samples: [48, 50, 64, 66, 86, 94, 103, 120, 121, 127, 137, 139, 140, 166, 167, 186, 193, 195,
208, 213, 218, 230, 240, 269, 273, 280, 311, 317, 325, 331, 364, 371, 372, 379, 402, 404,
409, 416, 429, 444]

\caption{List of duplicated rows.}
\end{table}

One way to address the dupicates is to use a supervised learning method to train a model on the non-duplicated rows and then predict the labels for the duplicated rows. Once we obtain these label predictions, we compare them with the duplicates' labels and see which ones match. If one does, then we keep that row and discard the other. If both match, we drop one of them to avoid having duplicates, even correctly labelled ones. And if none match, we drop both rows.

Using this method, the following rowws were dropped:

\begin{table}[h]
    \centering
    Samples: [29, 82, 100, 116, 118, 146, 172, 209, 219, 259, 290, 296, 350, 358, 381, 395]
    \caption{List of duplicated rows.}
\end{table}

Leaving 16 duplicated rows but correctly labelled. In this case we dropped

\bibliographystyle{plain}
\bibliography{refs.bib}


\end{document}