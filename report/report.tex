\documentclass[12pt]{report} % Increased the font size to 12pt
\usepackage{epigraph}
\usepackage{geometry}

% Optional: customize the style of epigraphs
\setlength{\epigraphwidth}{0.5\textwidth} % Adjust the width of the epigraph
\renewcommand{\epigraphflush}{flushright} % Align the epigraph to the right
\renewcommand{\epigraphrule}{0pt} % No horizontal rule
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage{hyperref} % Added for hyperlinks
\usepackage{listings} % Added for code listings
\usepackage{color}    % Added for color definitions
\usepackage[super]{nth} 
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{cite}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Define the graphics path
%\graphicspath{{./Plots/}}

% Define the header and footer for general pages
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead{} % Initially, the header is empty
\fancyfoot[C]{\thepage} % Page number at the center of the footer
\renewcommand{\headrulewidth}{0pt} % No header line on the first page of chapters
\renewcommand{\footrulewidth}{0pt} % No footer line

% Define the plain page style for chapter starting pages
\fancypagestyle{plain}{%
  \fancyhf{} % Clear all header and footer fields
  \fancyfoot[C]{\thepage} % Page number at the center of the footer
  \renewcommand{\headrulewidth}{0pt} % No header line
}

% Apply the 'fancy' style to subsequent pages in a chapter
\renewcommand{\chaptermark}[1]{%
  \markboth{\MakeUppercase{#1}}{}%
}

% Redefine the 'plain' style for the first page of chapters
\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyfoot[C]{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

% Header settings for normal pages (not the first page of a chapter)
\fancyhead[L]{\slshape \nouppercase{\leftmark}} % Chapter title in the header
\renewcommand{\headrulewidth}{0.4pt} % Header line width on normal pages

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}
% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% Definition of the tcolorbox for definitions
\newtcolorbox{definitionbox}{
  colback=red!5!white,
  colframe=red!75!black,
  colbacktitle=red!85!black,
  title=Definition:,
  fonttitle=\bfseries,
  enhanced,
}

% Definition of the tcolorbox for remarks
\newtcolorbox{remarkbox}{
  colback=blue!5!white,     % Light blue background
  colframe=blue!75!black,   % Darker blue frame
  colbacktitle=blue!85!black, % Even darker blue for the title background
  title=Remark:,            % Title text for remark box
  fonttitle=\bfseries,      % Bold title font
  enhanced,
}

% Definition of the tcolorbox for examples
\newtcolorbox{examplebox}{
  colback=green!5!white,
  colframe=green!75!black,
  colbacktitle=green!85!black,
  title=Example:,
  fonttitle=\bfseries,
  enhanced,
}

% Definitions and examples will be put in these environments
\newenvironment{definition}
    {\begin{definitionbox}}
    {\end{definitionbox}}

\newenvironment{example}
    {\begin{examplebox}}
    {\end{examplebox}}

\geometry{top=1.5in} % Adjust the value as needed
% ----------------------------------------------------------------------------------------


\title{S1 Principles of Data Science Coursework Report}
\author{CRSiD: tmb76}
\date{University of Cambridge}

\begin{document} 

\maketitle

\chapter*{Section A}

\section*{Question 1}

\subsection*{(a)}

To explore the data, it needs to be visualised in some way. This dataset being high-dimensional with 500 dimensions in the feature-space, makes visualisation tough. Thus, each feature is visualised separately with density plot. Densities of the first 20 features are obtained and shown below (Figure 1). The main observation is that some features follow very similar distributions. A rough assumption is thus that there are overall 3\-4 groups of highly correlated features in the entire dataset. The most common one has a very strong peak at 0, with a much smaller one at around 1. Another is bimodal, with peaks at 0 and 3. And the last one is also bimodal, with peaks at 0 and a stonger one at approximately 4. If the features can be grouped then one could reduce the dimensionality of the feature-space to 1 representative feature from each group.

\begin{figure}[hp]
    \centering
    \includegraphics[width=0.8\textwidth]{../Plots/A_Q1a.png}
    \caption{ \textbf{Density plots of the first 20 features, grouped by similarity}. The x-axis was set to the same scale for all plots, for easier comparison. The y-axis is however different for all plots.}
\end{figure}

\newpage

\subsection*{(b)}

Thus, Principle Component Analysis (PCA) is conducted on the dataset. PCA enables one to derive a lower-dimensional set of features from the full \textbf{A} dataset. This is done by finding the direction in which the observations have the greatest variance, for the full feature space\cite[pp. 255-257]{james2013introduction}. This is done on the entire dataset, not just the first 20 features. Using \texttt{scikit-learn}'s PCA function, getting the first 2 Principles Components (PC). 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q1b.png}
    \caption{\textbf{Scatter plot of the dataset after PCA.} Density contours are also shown.}
\end{figure}

From Figure 2, the bimodal separation seen in some of the first 20 features is seen again in the 1$^{st}$ PC.\ The 2$^{nd}$ PC also resembles the more commonly observed feature in Figure 1, with a very strong peak and a much weaker one at a slightly higher value, more from the right-hand side group. This is hard to see in the scatter plot, but the density contours do show the negative skewness of the data's distribution for that PC.\ It is clear that the PCA transformed data is linearly separable into 2 groups.

\subsection*{(c)}

From this, clustering can be run to try and see how they would compare to the distribution observed. The \texttt{scikit-learn} KMeans clustering algorithm was used, with all parameters set to their default values, apart from the random state.
The default number of clusters is 8\cite{kmeans_sklearn}. Now from Figure 1 and Figure 2, it seems unlikely that there are 8 clusters in the data. This is one of the risks of K-means clustering as it will ``find'' 8 clusters, even if there are not 8 clusters in the data. To assess if ``default'' K-means clustering performs adequatly, the data is split into 2. For both these splits, clustering is run and then applied to the other split.  
The reason this can be done is that K-means clustering's main result is the position of points called centroids, which are the geometrical centres of each clusters (when defining the centroids from the feature means). In terms of their significance they are each the most representative points of each cluster\cite[p. 243]{sklearn_book}. One of the \texttt{scikit-learn} K-means output is those \texttt{cluster\_centers\_}. Then when using the \texttt{.predict{}} method after fitting the model, i.e.\ clustering one half of the data, the other half can be clustered base on the distance of each point to the centroids. This is done for both splits of the data and the results are shown in the tables below:

\begin{table}[h]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c| }
    \hline
    \textbf{Clusters} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} \\ 
    \hline
    \textbf{1} & 0 & 72 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \hline
    \textbf{2} & 1 & 0 & 1 & 0 & 4 & 1 & 2 & 2 \\
    \hline
    \textbf{3} & 0 & 0 & 2 & 0 & 0 & 0 & 0 & 0 \\
    \hline
    \textbf{4} & 2 & 0 & 6 & 0 & 6 & 8 & 14 & 11 \\
    \hline
    \textbf{5} & 1 & 0 & 1 & 1 & 2 & 1 & 1 & 0 \\
    \hline
    \textbf{6} & 0 & 65 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \hline

\end{tabular}
\caption{Contingency table for the 2 clusterings on the first split of the data.}
\end{table}


\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c| }
        \hline
        \textbf{Clusters} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} \\ 
        \hline
        \textbf{1} & 0 & 81 & 0 & 0 & 0 & 0 & 54 & 0 \\
        \hline
        \textbf{2} & 0 & 0 & 0 & 3 & 0 & 0 & 0 & 0 \\
        \hline
        \textbf{4} & 1 & 0 & 5 & 1 & 3 & 3 & 0 & 1 \\
        \hline
        \textbf{5} & 0 & 0 & 1 & 0 & 2 & 1 & 0 & 1 \\
        \hline
        \textbf{6} & 0 & 0 & 7 & 5 & 14 & 9 & 0 & 2 \\
        \hline
        \textbf{7} & 0 & 0 & 3 & 0 & 2 & 2 & 0 & 3 \\
        \hline
    
    \end{tabular}
    \caption{Contingency table for the 2 clusterings on the second split of the data.}
\end{table}

\subsection*{(d)}

As can be seen, some clusters are empty. This can be explained by the initialization of the centroids. Indeed, K-means clustering has to place the centroids somewhere at the start, and they are then updated based on which points are closest to it and their mean. If the centroid happened to have no points closer to it than other centroids at the start, it then stays where it is for the whole clustering process\cite{kmeans_sklearn}. Overall, the fact that a too large cluster number was selected for the model means there is little agreement between clusters (Table 1). Note the cluster numbers are assigned ``randomly'' and so one could have the exact same clusters but one is cluster 4 and the other cluster 7. Even considering this, we'd expect only have 8 large numbers, each on a different row and column. Seeing there are numerous small groups of agreeing clusterings, this is not the case and the clustering is not very stable. Stability here refers to how constant the clusters are when clustering the same or very similar datasets. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../plots/A_Q1d_silhouette.png}
    \caption{\textbf{Silhouette scores for a few different number of clusters}}
\end{figure}

Thus, clustering is done again for a lower number of clusters. Based on the visualisation of the data, Fig 2, and Figure 3 which shows the silhouette score (a measure of how well the data is clustered)\cite[pp. 247-250]{sklearn_book}, a number of 2 clusters is chosen. And the following contingency table is obtained:

\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c|c| }
        \hline
        \textbf{Clusters} & \textbf{0} & \textbf{1}\\ 
        \hline
        \textbf{0} & 0 & 135\\
        \hline
        \textbf{1} & 69 & 0 \\
        \hline
    \end{tabular}
    \caption{\textbf{Contingency table comparing both clusterings for k=2 clusters}}
    \end{table}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q1d_2clusters_A1.png}
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q1d_2clusters_A2.png}
    \caption{\textbf{Scatter plot of the 2 clusterings} The colors indicate the cluster membership.}
\end{figure}


\subsection*{(e)}

Here, with the 2 clusters, both models agree on how to separate the data, as can be seen in Figure 4 and Table 3. 

Conducting PCA before and after clustering is useful in both cases. One gives a clue as to how many clusters one should use in the K-means training and the other enables one to visualise the clustering results. One additional thing is that the K-means could have been done on the PCA reduced dataset, which would have been a 2D dataset. The impact on performance in that case is not completely clear, but usually in a high dimensionality case, PCA helps avoid the Curse of Dimensionality. This Curse is data becoming very sparse as the number of dimensions increases\cite[p. 217]{sklearn_book}. This means distance measurements like the ones in K-means can run into some difficulty. And PCA transforming the data can help avoid some of those issues.

\newpage

\section*{Question 2}

The same dataset as in A is given, but with duplicated observations and missing labels introduced.

\subsection*{(a)}

First, the frequency of the labels is summarised in the table below (Table 2):

\begin{table}[h]
\centering
\begin{tabular}{ |c|c| }
    \hline
    \textbf{Label} & \textbf{Frequency} \\
    \hline
    \textbf{1.0} & 179 \\ 
    \hline
    \textbf{2.0} & 157 \\
    \hline
    \textbf{4.0} & 72 \\
    \hline
\end{tabular}
\caption{Frequency of the labels in the \textbf{B} dataset.}
\end{table}

\subsection*{(b)}

Taking only the feature columns, the following rows were identified as duplicates:

\begin{table}[h]
\centering
Samples: [27, 29, 43, 45, 65, 73, 82, 99, 100, 106, 116, 118, 119, 145, 146, 165, 172, 174,
187, 192, 197, 209, 219, 248, 252, 259, 290, 296, 304, 310, 343, 350, 351, 358, 381, 383,
388, 395, 408, 423]

\caption{List of the 40 duplicated rows.}
\end{table}

One way to address the dupicates is to use a supervised learning method to train a model on the non-duplicated rows' labels and then predict them for the duplicated rows. Once we obtain these label predictions, we compare them with the duplicates' labels and see which ones match. If one does, then we keep that row and discard the other. If both match, we keep only one of them to avoid having duplicates, even correctly labelled ones. And if none match, we drop both rows.

Using this method, the following rows were dropped:

\begin{table}[h]
    \centering
    Samples: [29, 82, 100, 116, 118, 146, 172, 209, 219, 259, 290, 296, 350, 358, 381, 395]
    \caption{List of mislabelled dropped rows}
\end{table}

Leaving 16 duplicated rows but correctly labelled, meaning half of which need to be dropped. The following samples were dropped: [106, 145, 192, 248, 252, 310, 388, 423]. 

\subsection*{(c)}

Missing labels cases are sometimes referred to as a semi-supervised learning case\cite[p. 24]{james2013introduction}. Here are 2 ways observations with missing labels can be dealt with. The first is model based imputation. Similarly to the previous question, this relies on training a machine learning model on a complete version of the dataset (sometimes a subset), and then predicting the labels for the observations with missing labels. The choice of model here is broad since the total number of missing values is not too great, and multiple models would be appropriate. This method takes into account relationships between features though runs the risk of an invalid prediction and therefore affecting the result of any model training conducted subsequently. Another more brute-force approach is a simple removal of those observations. This has the advantage of simplicity though brings loss of information, and becomes inappropriate in a scenario where many observations have missing labels. For that second method especially, it is important to consider why the labels are missing. They can be Missing At Random (MAR), meaning there is no underlying reason for that value missing. On the other hand they could be Missing Not At Random (MNAR), meaning that the data missing, sometimes combined with the values of the features for that observation, carry an important meaning. In that case, dropping these observations would be an important loss of information\cite[pp. 515-516]{james2013introduction}.

\subsection*{(d)}

Here, a model based imputation method is used, assuming the labels are MAR. A multinomial logistic regression is trained on the data without the rows with missing labels. The 20 rows identified to have missing values are dropped, and the model is trained on the remaining data. Testing returns an accuracy of 0.847. The model is then used to predict the labels for the rows missing them. We obtain new frequencies for the labels: 

\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c| }
        \hline
        \textbf{Label} & \textbf{Frequency} \\
        \hline
        \textbf{1.0} & 187 \\ 
        \hline
        \textbf{2.0} & 162 \\
        \hline
        \textbf{4.0} & 64 \\
        \hline
    \end{tabular}
    \caption{Frequency of the labels in the \textbf{B} dataset, after missing label prediction.}
    \end{table}

Table 7 shows a slight tendency to predict labels 1.0 and 2.0 compared to 4.0, though the observations with missing values do not have to follow the original proportions, since we assumed the values were MAR.

\section*{Question 3}

\subsection*{(a)}

In this dataset, values in various observations and features are missing. 5 observations have missing values. These are samples: [137, 142, 230, 262, 388]. For each of these rows, the following features are missing:

\begin{center}
    Missing Features: [Fea58, Fea142, Fea150, Fea233, Fea269, Fea299, Fea339, Fea355, Fea458, Fea466, Fea491]
\end{center}

\subsection*{(b)}

Again, one can deal with these missing values in a number of ways.  
One is to use model-based imputation, as in Q2. An example of that is using a k-Nearest Neighbour imputing method. The model looks at the $k$ closest observations to the one with missing values and takes the mean of the values for that feature. Again, this captures relationships between the features, and one can select the number $k$ as they wish. However, it does assume a linear relationship.  
Another is a simple constant value-based imputation. This usually just takes the mean or median value of that feature and imputs it where values are missing for that feature. This is a simple approach where you can avoid dropping values but it ignores any relationship between features and as a result some loss of information occurs.  
One way to improve this result is by conducting multiple imputations. Choosing an imputation model, one can repeat the imputation, obtaining a mean and uncertainty of their results. Moreover, imputing multiple times means greater adaptability to the data\cite{reiter2020}.

\subsection*{(c)}

Here, a k-Nearest Neighbour imputation is used. As said above, this takes into account feature relationships while not being too computationally expensive. The number of neighbours $k$ is set to 10. This is from training a kNN model in Q2 and found an optimal number of neighbours to be 10. Comparing the distribution within the concerned features before and after imputation, Figure 5 is obtained. As can be seen the distributions show little change, except for small dips at certain points, where the original has lower density than the imputed distribution.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q3c_1.png}
    \caption{\textbf{Density plots of features before and after the imputation}}
\end{figure}

\subsection*{(d)}

To detect outliers, one can calculate the z-scores of each features by standardizing the data. This is done by shifitng/subtracting feature values by their mean and dividing by their standard deviation\cite[p. 73]{sklearn_book}. This thus expresses each value by how many standard deviations they are from the mean. By then setting a threshold of 3, values further than 3 standard deviations can be identified. Here, outliers were found in numerous rows and over all features. Over 2900 were found in the total of 204,000 data points in the dataset.

\subsection*{(e)}

Here, a KNN-imputer was used. By training the model on the data with the outliers still there, the outliers can then be dropped by setting them to NaN. Then using the imputer, the missing values/outliers are imputed/corrected with new values that reflect the overall dataset rather than the outliers. After imputation, the number of outliers is reduced to 1385. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q3c_2.png}
    \caption{\textbf{Density plots of features before and after outlier correction}}
\end{figure}

Some of those distributions are not plotted as the variance of those features is now 0, after the imputation. Figure 6 shows that some distributions have been narrowed, which is what is expected from correcting outliers.

\section*{Question 4}

\subsection*{(a)}

A single decision tree is grown through recursive binary splitting of the data. It is a greedy search as at every node, the splitting is only made according to what the best split is at that time\cite[pp. 337-338]{james2013introduction}. That is described by some criterion, like the Gini Index. The Gini index is a measure of the node "purity", how much of one class the node contains\cite[pp. 338-339]{james2013introduction}. Once the tree is fully grown down to having only totally pure nodes at the end, one can prune it, removing some of the last nodes to avoid having an overtrained tree. In bagging, multiple trees are grown independently on bootstrapped samples of the dataset and combined to reduce the high variance issues single decision trees have\cite[p. 343]{james2013introduction}. Random forests are built in a similar way to bagging but as a tree is built on a bootstrapped sample, at each split the tree has to make, it is only performed on a subset of the features. This decorrelates the trees, and let's them explore a lot more possible splits\cite[354]{james2013introduction}. This sort of helps counteract the greedy nature of the tree. When using a random forest there are a few hyperparameters one can tune to control how each tree is grown. First is the criterion used, as there are options other than the Gini index. Second, the number of features that the tree has to split from at each node. It is usually set to the squareroot of the total number of features.

\subsection*{(b)}

In preparation to training a classifier on the data, some preprocessing was done. The first thing done was to count missing values, and none were found. Then, checks were run for duplicates and none were found either. Now, checks were made for zero-variance features and the following features were identified and dropped:

\begin{center}
    0 Variance features: [Fea49, Fea66, Fea94, Fea97, Fea106, Fea109, Fea125, Fea129, Fea151, Fea152, Fea187, Fea189, Fea223, Fea224, Fea238, Fea264, Fea293, Fea324, Fea384, Fea432, Fea440, Fea450, Fea463, Fea543, Fea636, Fea666, Fea680, Fea701, Fea707, Fea728, Fea729, Fea750, Fea785, Fea787, Fea808, Fea830, Fea846, Fea930, Fea943, Fea945, Fea973, Fea982]
\end{center}

Further, with the average variance being 0.618, 16 rows were found with near-zero variance and were also dropped:

\begin{center}
    Near-zero Variance features: [Fea747, Fea730, Fea57, Fea544, Fea4, Fea154, Fea597, Fea610, Fea19, Fea855, Fea347, Fea843, Fea499, Fea231, Fea404, Fea546]
\end{center}

Outliers were then dealt with

Finally, 2 pairs of highly correlated features (> 0.9) were found and one of each pairs were dropped: Feature 300 and 345, and Feature 869 and 954.

Because the Random Forest classifier is robust to outliers, outliers are not dealt with\cite[pp. 346-347]{james2013introduction}.


\subsection*{(c)}

The random forest classifier was trained on default parameters: 100 trees, Gini index splitting criteria, no max depth set, and the max number of features to split from set to the squareroot of the total number of features.  

Training this model, a test set classification error of 0.04 is found.

\subsection*{(d)}

This number of trees in the random forest can be optimised using an Out-Of-Bag (OOB) score. This works from the fact each tree is built for a bootstrapped sub-sample of the dataset. Leaving an "Out-Of-Bag" sample the trees for which that sample was OOB can be tested upon. An OOB classification error can thus be obtained, for each number of trees\cite[p. 345]{james2013introduction}.  


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/B_Q4d_1.png}
    \caption{\textbf{OOB error vs number of trees} Density contours are also shown.}
\end{figure}

Thus, the optimal number of trees is around 150, where the OOB error stabilizes, and going further only makes the model more computationally expensive.

Re-running the model for the optimal tree number, the test-classification error now is 0.04.


\subsection*{(e)}

One characteristic of the random forest classifier is it can give us the relative importance of features. It is computed as how much of a reduction in the Gini index (or other used criterion) is brought by that feature. Were it not for the bootstrapping and subset of features to split from, there would not be enough information for that measure to be computed accurately\cite[pp. 345-346]{james2013introduction}. In this case, we obtained the following plot, with the 50 most important features:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{../Plots/B_Q4e_1.png}
    \caption{\textbf{Random Forest Feature Importance}, for the 50 most important features}
\end{figure}

We see that feature importance decreases very rapidly before steadily decreasing slowly. Taking the first 20, another random forest model can be trained. With this model, a test set classification error of 0.0699 was obtained. As expected, the performance is slightly lower but considering the amount by which computational complexity was decreased, this is a very positive result.


\subsection*{(f)}

Here, the previous questions (b, c, e) are reproduced using another supervised learning method. The chosen method is Support Vector Machines (SVM), as it is quite good when the data is high dimensional, and even when there are more dimensions than samples \cite{svm_sklearn}. However, in order to get the feature importances back from the SVM model, one needs to use a linear kernel. 

The data was preprocessed in this same way as for the Random Forest Classifier. Duplicates, low-variance, and highly correlated rows and features are dealt with, and outliers are left alone since the SVM algorithm is robust to them.  

Running this, a test set classification error of 0.0799 was obtained. And using the \texttt{coeff\_} output of the model, the feature importances were obtained and plotted in figure 7. As one cans see there is some agreement between the 2 models, with the most important feature being the same and a few features with relatively similar importance (Fea36, Fea354,\ldots)  
Training the SVM model on just the most important feature, the test set classification error roughly doubles but stays indicative of good performance.

\begin{figure}{h}
    \centering
    \includegraphics[width=0.5\textwidth]{../Plots/B_Q4e_2.png}
    \caption{\textbf{SVM Feature Importance}, for the 50 most important features}
\end{figure}

\newpage

\section*{Question 5}

\subsection*{(a)}

Two different clustering techniques are explored in this question. One of them will be K-means which has been already discussed partly. It will place a desired number of initial centroids in the feature space and update their position based on cluster members until convergence. The two main outputs are then the cluster centroids' final location and the cluster memberships of all the datasets' points.  
The second technique will Gaussian Mixture Models. The idea behind this is to assume the entire data is distributed as the sum of multiple multivariate Gaussian distributions with unknown parameters\cite[p. 260]{sklearn_book}. The main outputs for this clustering technique are the parameters of those gaussians (means and covariance matrices), and the weights for each of those Gaussians to be used in the sum.  
Both of these techniques' output can be used on a dataset to assign cluster memberships to each observation. This is done by assigning each observation to the cluster whose centroid is closest to it for K-means, and by assigning each observation to the Gaussian which has the highest probability of generating that observation for GMM. One property of the GMM is that it can be used to generate new observations, by sampling from the weighted Gaussians. This makes the GMM a generative model.  

Here, the data was preprocessed in the same way as in Q4 with the addition of dealing with outliers, using the same method as in Q3.
Using these methods, clusterings are obtained both for the same data which makes comparison easier. The number of clusters is set to 3 for both methods, which is based on the visualisation of the data. The results are shown in

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/B_Q5a_silhouette.png}
    \caption{\textbf{Silhouette scores for the Baseline Dataset}}
\end{figure}

For each clustering technique, label frequencies were obtained.

\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c| }
        \hline
        \textbf{Label} & \textbf{Frequency} \\
        \hline
        \textbf{0} & 103 \\ 
        \hline
        \textbf{1} & 173 \\
        \hline
        \textbf{3} & 224 \\
        \hline
    \end{tabular}
    \caption{Frequency of the labels for the K-means clustering of \textbf{Baseline} dataset}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c| }
        \hline
        \textbf{Label} & \textbf{Frequency} \\
        \hline
        \textbf{0} & 169 \\ 
        \hline
        \textbf{1} & 176 \\
        \hline
        \textbf{3} & 155 \\
        \hline
    \end{tabular}
    \caption{Frequency of the labels for the GMM clustering of \textbf{Baseline} dataset}
\end{table}

And the following contingency table comparing the 2 clusterings was obtained:

\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c|c|c| }
        \hline
        \textbf{Clusters} & \textbf{0} & \textbf{1} & \textbf{2}\\ 
        \hline
        \textbf{0} & 53 & 45 & 5\\
        \hline
        \textbf{1} & 4 & 68 & 101\\
        \hline
        \textbf{2} & 112 & 63 & 49\\
        \hline
    \end{tabular}
    \caption{Contingency table for the 2 clusterings on the \textbf{Baseline} dataset. Rows are K-means, columns are GMM.}
\end{table}

Thus, the clusterings are not very stable. There are 2 larger groups (112, 101) which show some agreement for the "cores" of 2 clusters though 5 groups of consequent size (53, 45, 68, 63, 49) which clearly show some ambiguity as to what cluster some points should belong to. 

\subsection*{(b)}

Similar to Q4, a random forest classifier is trained to determine feature importances. However, where the problem was supervised and the true labels were known, here the labels obtained from the clusterings are used. Thus, 2 models are trained and the following feature importances were derived.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{../Plots/B_Q4e_3.png}
    \includegraphics[width=0.4\textwidth]{../Plots/B_Q4e_4.png}
    \caption{\textbf{Random Forest Feature Importance}, for the 50 most important features, from K-means clustering (left) and GMM clustering (right)}
\end{figure}

As one can see, there is some agreement between the 2 on what features are most important. With each set of most important features, the clustering is re-run for each technique based on each top 40 features only. The following contingency tables compare the 2 top 40 features models together and for each technique, the top 40 and the total features clustering.

\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c|c|c| }
        \hline
        \textbf{Clusters} & \textbf{0} & \textbf{1} & \textbf{2}\\ 
        \hline
        \textbf{0} & 6 & 103 & 5\\
        \hline
        \textbf{1} & 17 & 53 & 79\\
        \hline
        \textbf{2} & 175 & 0 & 62\\
        \hline
    \end{tabular}
    \caption{\textbf{Contingency table for the 2 clusterings on the Baseline dataset.} For the top 40 most important features. Rows are K-means and columns are GMM.}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c|c|c| }
        \hline
        \textbf{Clusters} & \textbf{0} & \textbf{1} & \textbf{2}\\ 
        \hline
        \textbf{0} & 91 & 20 & 3\\
        \hline
        \textbf{1} & 11 & 130 & 8\\
        \hline
        \textbf{2} & 1 & 23 & 213\\
        \hline
    \end{tabular}
    \caption{\textbf{Contingency table for the top 40 and total features clustering on the Baseline dataset.} For K-means. The rows are the top 40 features clustering and the columns are the total features clustering.}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c|c|c| }
        \hline
        \textbf{Clusters} & \textbf{0} & \textbf{1} & \textbf{2}\\ 
        \hline
        \textbf{0} & 168 & 9 & 21\\
        \hline
        \textbf{1} & 3 & 134 & 19\\
        \hline
        \textbf{2} & 43 & 10 & 93\\
        \hline
    \end{tabular}
    \caption{\textbf{Contingency table for the top 40 and total features clustering on the Baseline dataset.} For GMM. The rows are the top 40 features clustering and the columns are the total features clustering.}
\end{table}


\subsection*{(c)}

Having applied PCA to run the GMM clustering, the data can be visualised on the first 2 PCs. The following plots show the data colour coded according to cluster memberships for the total features clustering and the top 40 features clustering, each for the 2 clustering techniques.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/B_Q5c_total_clusters.png}
    \caption{\textbf{Scatter plot of cluster memberships.}For the total features clustering. K-means (left) and GMM (right).}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/B_Q5c_top40_clusters.png}
    \caption{\textbf{Scatter plot of cluster memberships.}For the top 40 features clustering. K-means (left) and GMM (right).}
\end{figure}

Here, the agreement can be seen between the 2 techniques. The top 40 features clustering is more ambiguous, with the clusters overlapping more, especially for K-means. The main issue with the 2 clustering techniques used is that they are both at risk of falling into local minima. This is especially true for K-means, which is very sensitive to the initialisation of the centroids\cite[p. 263]{sklearn_book}. This could be the reason for some of the disagreements. Futhermore, the Baseline dataset is quite high dimensional, with 1000 features.  

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/B_Q5c_2.png}
    \caption{\textbf{Scatter plot of the data color coded according to the most important feature.} K-means (left) and GMM (right).}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/B_Q5c_3.png}
    \caption{\textbf{Scatter plot of the data color coded according to the 2nd most important feature.} K-means (left) and GMM (right).}
\end{figure}

Figure 12 and 13 show the data colour coded according to the first and second most important features for both clustering techniques. The first most important feature seems to help in differentiating between the 2 right and left clusters, and the bottom cluster, comparing with figure 10 and 11. The second most important feature seems to help in differentiating between the right and left clusters, again looking at Figure 10 and 11.


\bibliographystyle{plain}
\bibliography{refs.bib}


\end{document}