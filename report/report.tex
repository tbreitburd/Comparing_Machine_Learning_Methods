\documentclass[12pt]{report} % Increased the font size to 12pt
\usepackage{epigraph}
\usepackage{geometry}

% Optional: customize the style of epigraphs
\setlength{\epigraphwidth}{0.5\textwidth} % Adjust the width of the epigraph
\renewcommand{\epigraphflush}{flushright} % Align the epigraph to the right
\renewcommand{\epigraphrule}{0pt} % No horizontal rule
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage{hyperref} % Added for hyperlinks
\usepackage{listings} % Added for code listings
\usepackage{color}    % Added for color definitions
\usepackage[super]{nth} 
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{cite}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Define the graphics path
%\graphicspath{{./Plots/}}

% Define the header and footer for general pages
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead{} % Initially, the header is empty
\fancyfoot[C]{\thepage} % Page number at the center of the footer
\renewcommand{\headrulewidth}{0pt} % No header line on the first page of chapters
\renewcommand{\footrulewidth}{0pt} % No footer line

% Define the plain page style for chapter starting pages
\fancypagestyle{plain}{%
  \fancyhf{} % Clear all header and footer fields
  \fancyfoot[C]{\thepage} % Page number at the center of the footer
  \renewcommand{\headrulewidth}{0pt} % No header line
}

% Apply the 'fancy' style to subsequent pages in a chapter
\renewcommand{\chaptermark}[1]{%
  \markboth{\MakeUppercase{#1}}{}%
}

% Redefine the 'plain' style for the first page of chapters
\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyfoot[C]{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

% Header settings for normal pages (not the first page of a chapter)
\fancyhead[L]{\slshape \nouppercase{\leftmark}} % Chapter title in the header
\renewcommand{\headrulewidth}{0.4pt} % Header line width on normal pages

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}
% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% Definition of the tcolorbox for definitions
\newtcolorbox{definitionbox}{
  colback=red!5!white,
  colframe=red!75!black,
  colbacktitle=red!85!black,
  title=Definition:,
  fonttitle=\bfseries,
  enhanced,
}

% Definition of the tcolorbox for remarks
\newtcolorbox{remarkbox}{
  colback=blue!5!white,     % Light blue background
  colframe=blue!75!black,   % Darker blue frame
  colbacktitle=blue!85!black, % Even darker blue for the title background
  title=Remark:,            % Title text for remark box
  fonttitle=\bfseries,      % Bold title font
  enhanced,
}

% Definition of the tcolorbox for examples
\newtcolorbox{examplebox}{
  colback=green!5!white,
  colframe=green!75!black,
  colbacktitle=green!85!black,
  title=Example:,
  fonttitle=\bfseries,
  enhanced,
}

% Definitions and examples will be put in these environments
\newenvironment{definition}
    {\begin{definitionbox}}
    {\end{definitionbox}}

\newenvironment{example}
    {\begin{examplebox}}
    {\end{examplebox}}

\geometry{top=1.5in} % Adjust the value as needed
% ----------------------------------------------------------------------------------------


\title{S1 Principles of Data Science Coursework Report}
\author{CRSiD: tmb76}
\date{University of Cambridge}

\begin{document} 

\maketitle

\chapter*{Section A}

\section*{Question 1}

\subsection*{(a)}

To explore the data, it needs to be visualised in some way. This dataset being high-dimensional with 500 dimensions in the feature-space, makes visualisation tough. Thus, each feature is visualised separately with density plot. Densities of the first 20 features are obtained and shown below (Figure 1). The main observation is that some features follow very similar distributions. A rough assumption is thus that there are overall 3\-4 groups of highly correlated features in the entire dataset. The most common one has a very strong peak at 0, with a much smaller one at around 1. Another is bimodal, with peaks at 0 and 3. And the last one is also bimodal, with peaks at 0 and a stonger one at approximately 4. If the features can be grouped then one could reduce the dimensionality of the feature-space to 1 representative feature from each group.

\begin{figure}[hp]
    \centering
    \includegraphics[width=0.8\textwidth]{../Plots/A_Q1a.png}
    \caption{ \textbf{Density plots of the first 20 features, grouped by similarity}. The x-axis was set to the same scale for all plots, for easier comparison. The y-axis is however different for all plots.}
\end{figure}

\newpage

\subsection*{(b)}

Thus, Principle Component Analysis (PCA) is conducted on the dataset. PCA enables one to derive a lower-dimensional set of features from the full \textbf{A} dataset. This is done by finding the direction in which the observations have the greatest variance, for the full feature space\cite[pp. 255-257]{james2013introduction}. This is done on the entire dataset, not just the first 20 features. Using \texttt{scikit-learn}'s PCA function, getting the first 2 Principles Components (PC). 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q1b.png}
    \caption{\textbf{Scatter plot of the dataset after PCA.} Density contours are also shown.}
\end{figure}

From Figure 2, the bimodal separation seen in some of the first 20 features is seen again in the 1$^{st}$ PC. SMTHG about linear sep.\ The 2$^{nd}$ PC also resembles the more commonly observed feature in Figure 1, with a very strong peak and a much weaker one at a slightly higher value, more from the right-hand side group. This is hard to see in the scatter plot, but the density contours do show the negative skewness of the data's distribution for that PC.\

\subsection*{(c)}

From this, clustering can be run to try and see how they would compare to the distribution observed. The \texttt{scikit-learn} KMeans clustering algorithm was used, with all parameters set to their default values, apart from the random state.
The default number of clusters is 8. Now from Figure 1 and figure 2, it seems unlikely that there are 8 clusters in the data. This is one of the risks of K-means clustering as it will ``find'' 8 clusters, even if there are not 8 clusters in the data. To assess if ``default'' K-means clustering performs adequatly, the data is split into 2. For both these splits, clustering is run and then applied to the other split.  
The reason this can be done is that K-means clustering's main result is the position of points called centroids, which are the geometrical centres of each clusters (when defining the centroids from the feature means). In terms of their significance they are each the most representative points of each cluster\cite[p. 243]{sklearn_book}. One of the \texttt{scikit-learn} K-means output is those \texttt{cluster\_centers\_}. Then when using the \texttt{.predict{}} method after fitting the model, i.e.\ clustering one half of the data, the other half can be clustered base on the distance of each point to the centroids. This is done for both splits of the data and the results are shown in the tables below:

\begin{table}[h]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c| }
    \hline
    \textbf{Clusters} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} \\ 
    \hline
    \textbf{0} & 0 & 67 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \hline
    \textbf{4} & 0 & 70 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \hline
    \textbf{5} & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
    \hline
    \textbf{6} & 1 & 0 & 11 & 2 & 9 & 11 & 26 & 3 \\
    \hline
    \textbf{7} & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
    \hline

\end{tabular}
\caption{Contingency table for the 2 clusterings on the first split of the data.}
\end{table}


\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c| }
        \hline
        \textbf{Clusters} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} \\ 
        \hline
        \textbf{1} & 76 & 0 & 0 & 0 & 59 & 0 & 0 & 0 \\
        \hline
        \textbf{2} & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 2 \\
        \hline
        \textbf{4} & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
        \hline
        \textbf{5} & 0 & 0 & 1 & 0 & 0 & 3 & 3 & 1 \\
        \hline
        \textbf{6} & 0 & 1 & 0 & 2 & 0 & 11 & 34 & 8 \\
        \hline
    
    \end{tabular}
    \caption{Contingency table for the 2 clusterings on the second split of the data.}
\end{table}

\subsection*{(d)}

As can be seen, some clusters are empty. This can be explained by the initialization of the centroids. Indeed, K-means clustering has to place the centroids somewhere at the start, and they are then updated based on which points are closest to it and their mean. If the centroid happened to have no points closer to it than other centroids at the start, it then stays where it is for the whole clustering process\cite{kmeans_sklearn}. Overall, the fact that a too large cluster number was selected for the model means there is little agreement between clusters (Table 1). Note the cluster numbers are assigned ``randomly'' and so one could have the exact same clusters but one is cluster 4 and the other cluster 7. Even considering this, we'd expect only have 8 large numbers, each on a different row and column. Seeing there are numerous small groups of agreeing clusterings, this is not the case and the clustering is not very stable. Stability here refers to how constant the clusters are when clustering the same or very similar datasets. 

Thus, clustering is done again for a lower number of clusters. Based on the visualisation of the data, Fig 2, and the fact the contingency tables show 3 main groups of samples for which the clusterings agree. The results are shown in Figure 3.  


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q1d_2clusters_A1.png}
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q1d_2clusters_A2.png}
    \caption{\textbf{Scatter plot of the 2 clusterings} The colors indicate the cluster membership.}
\end{figure}


\subsection*{(e)}

Here, with the 2 clusters, both models agree on how to separate the data. 

Conducting PCA before and after clustering is useful in both cases. One gives a clue as to how many clusters one should use in the K-means training and the other enables one to visualise the clustering results. One additional thing is that the K-means could have been done on the PCA reduced dataset, which would have been a 2D dataset. The impact on performance in that case is not completely clear.

\newpage

\section*{Question 2}

The same dataset as in A is given, but with duplicated observations and missing labels introduced.

\subsection*{(a)}

First, the frequency of the labels is summarised in the table below (Table 2):

\begin{table}[h]
\centering
\begin{tabular}{ |c|c| }
    \hline
    \textbf{Label} & \textbf{Frequency} \\
    \hline
    \textbf{1.0} & 179 \\ 
    \hline
    \textbf{2.0} & 157 \\
    \hline
    \textbf{4.0} & 72 \\
    \hline
\end{tabular}
\caption{Frequency of the labels in the \textbf{B} dataset.}
\end{table}

\subsection*{(b)}

Taking only the feature columns, the following rows were identified as duplicates:

\begin{table}[h]
\centering
Samples: [48, 50, 64, 66, 86, 94, 103, 120, 121, 127, 137, 139, 140, 166, 167, 186, 193, 195,
208, 213, 218, 230, 240, 269, 273, 280, 311, 317, 325, 331, 364, 371, 372, 379, 402, 404,
409, 416, 429, 444]

\caption{List of the 40 duplicated rows.}
\end{table}

One way to address the dupicates is to use a supervised learning method to train a model on the non-duplicated rows' labels and then predict them for the duplicated rows. Once we obtain these label predictions, we compare them with the duplicates' labels and see which ones match. If one does, then we keep that row and discard the other. If both match, we keep only one of them to avoid having duplicates, even correctly labelled ones. And if none match, we drop both rows.

Using this method, the following rows were dropped:

\begin{table}[h]
    \centering
    Samples: [29, 82, 100, 116, 118, 146, 172, 209, 219, 259, 290, 296, 350, 358, 381, 395]
    \caption{List of mislabelled dropped rows}
\end{table}

Leaving 16 duplicated rows but correctly labelled, meaning half of which need to be dropped. The following samples were dropped: [106, 145, 192, 248, 252, 310, 388, 423]. 

\subsection*{(c)}

Missing labels cases are sometimes referred to as a semi-supervised learning case\cite[p. 24]{james2013introduction}.Here are 2 ways observations with missing labels can be dealt with. The first is model based imputation. Similarly to the previous question, this relies on training a machine learning model on a complete version of the dataset (sometimes a subset), and then predicting the labels for the observations with missing labels. The choice of model here is broad since the total number of missing values is not too great, and multiple models would be appropriate. This method takes into account relationships between features though runs the risk of an invalid prediction and therefore affecting the result of any model training conducted subsequently. Another more brute-force approach is a simple removal of those observations. This has the advantage of simplicity though brings loss of information, and becomes inappropriate in a scenario where many observations have missing labels. For that second method especially, it is important to consider why the labels are missing. They can be Missing At Random (MAR), meaning there is no underlying reason for that value missing. On the other hand they could be Missing Not At Random (MNAR), meaning that the data missing, sometimes combined with the values of the features for that observation, carry an important meaning. In that case, dropping these observations would be an important loss of information\cite[pp. 515-516]{james2013introduction}.

\subsection*{(d)}

Here, a model based imputation method is used, assuming the labels are MAR. A multinomial logistic regression is trained on the data without the rows with missing labels. Those 20 rows are dropped, the model is trained and testing returns an accuracy of 0.847. The model is then used to predict the labels for the rows missing them. We obtain new frequencies for the labels: 

\begin{table}[h]
    \centering
    \begin{tabular}{ |c|c| }
        \hline
        \textbf{Label} & \textbf{Frequency} \\
        \hline
        \textbf{1.0} & 187 \\ 
        \hline
        \textbf{2.0} & 162 \\
        \hline
        \textbf{4.0} & 64 \\
        \hline
    \end{tabular}
    \caption{Frequency of the labels in the \textbf{B} dataset, after missing label prediction.}
    \end{table}

Table 6 shows a slight tendency to predict labels 1.0 and 2.0 compared to 4.0, though the observations with missing values do not have to follow the original proportions.

\section*{Question 3}

\subsection*{(a)}

In this dataset, values in various observations and features are missing. 5 observations have missing values. These are samples: [137, 142, 230, 262, 388]. For each of these rows, the following features are missing:

\begin{center}
    Missing Features: [Fea58, Fea142, Fea150, Fea233, Fea269, Fea299, Fea339, Fea355, Fea458, Fea466, Fea491]
\end{center}

\subsection*{(b)}

Again, one can deal with these missing values in a number of ways.  
One is to use model-based imputation, as in Q2. An example of that is using a k-Nearest Neighbour imputing method. The model looks at the $k$ closest observations to the one with missing values and takes the mean of the values for that feature. Again, this captures relationships between the features, and one can select the number $k$ as they wish. However, it does assume a linear relationship.  
Another is a simple constant value-based imputation. This usually just takes the mean or median value of that feature and imputs it where value are missing for that feature. This is a simple approach where you can avoid dropping values but it ignores any relationship between features and as a result some loss of information occurs.  
One way to improve this result is by conducting mulitple imputations. Choosing an imputation model, one can repeat the imputation, obtaining a mean and uncertainty of their results. Moreover, imputing multiple times means greater adaptability to the data\cite{reiter2020}.

\subsection*{(c)}

Here, a k-Nearest Neighbour imputation is used. As said above, this takes into account feature relationships while not being too computationally expensive. The number of neighbours $k$ is set to 10. This is from training a kNN model in Q2 and found an optimal number of neighbours to be 10. Comparing the distribution within the concerned features before and after imputation is the plot below:


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/A_Q3c.png}
    \caption{\textbf{Density plots of features before and a} Density contours are also shown.}
\end{figure}

\subsection*{(d)}

To detect outliers, one can calculate the z-scores of each features by standardizing the data. This is done by shifitng/subtracting feature values by their mean and dividing by their standard deviation\cite[p. 73]{sklearn_book}. This thus expresses each value by how many standard deviations they are from the mean. By then setting a threshold of 3, values furhter than 3 standard deviations can be identified.

\subsection*{(e)}

Multiple K-NN imputation.



\section*{Question 4}

\subsection*{(a)}

A single decision tree is grown through recursive binary splitting of the data. It is a greedy search as at every node, the splitting is only made according to what the best split is at that time\cite[pp. 337-338]{james2013introduction}. That is described by some criterion, like the Gini Index. The Gini index is a measure of the node "purity", how much of one class the node contains\cite[pp. 338-339]{james2013introduction}. Once the tree is fully grown down to having only totally pure nodes at the end, one can prune it, removing some of the last nodes to avoid having an overtrained tree. In bagging, multiple trees are grown independently on bootstrapped samples of the dataset and combined to reduce the high variance issues single decision trees have\cite[p. 343]{james2013introduction}. Random forests are built in a similar way to bagging but as a tree is built on a bootstrapped sample, at each split the tree has to make, it is only performed on a subset of the features. This decorrelates the trees, and let's them explore a lot more possible splits\cite[354]{james2013introduction}. This sort of helps counteract the greedy nature of the tree. When using a random forest there are a few hyperparameters one can tune to control how each tree is grown. First is the criterion used, as there are options other than the Gini index. Second, the number of features that the tree has to split from at each node. It is usually set to the squareroot of the total number of features.

\subsection*{(b)}

In preparation to training a classifier on the data, some preprocessing was done. The first thing done was to count missing values, and none were found. Then, checks were run for duplicates and none were found either. Now, checks were made for zero-variance features and the following features were identified and dropped:

\begin{center}
    0 Variance features: [Fea49, Fea66, Fea94, Fea97, Fea106, Fea109, Fea125, Fea129, Fea151, Fea152, Fea187, Fea189, Fea223, Fea224, Fea238, Fea264, Fea293, Fea324, Fea384, Fea432, Fea440, Fea450, Fea463, Fea543, Fea636, Fea666, Fea680, Fea701, Fea707, Fea728, Fea729, Fea750, Fea785, Fea787, Fea808, Fea830, Fea846, Fea930, Fea943, Fea945, Fea973, Fea982]
\end{center}

Further, with the average variance being 0.618, 16 rows were found with near-zero variance and were also dropped:

\begin{center}
    Near-zero Variance features: [Fea747, Fea730, Fea57, Fea544, Fea4, Fea154, Fea597, Fea610, Fea19, Fea855, Fea347, Fea843, Fea499, Fea231, Fea404, Fea546]
\end{center}

Outliers were then dealt with

Finally, 2 pairs of highly correlated features (> 0.9) were found and one of each pairs were dropped: Feature 300 and 345, and Feature 869 and 954.


\subsection*{(c)}

The random forest classifier was trained on default parameters: 100 trees, Gini index splitting criteria, no max depth set, and the max number of features to split from set to the squareroot of the total number of features.  

Training this model, a test set classification error of 0.05 is found.

(d) Optimise the algorithm with respect to the number of trees in the random forest. You should be able to do this without explicitly performing cross-validation.

\subsection*{(d)}

This number of trees in the random forest can be optimised using an Out-Of-Bag (OOB) score. This works from the fact each tree is built for a bootstrapped sub-sample of the dataset. Leaving an "Out-Of-Bag" sample the trees for which that sample was OOB can be tested upon. An OOB classification error can thus be obtained, for each number of trees\cite[p. 345]{james2013introduction}.  


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../Plots/B_Q4d_1.png}
    \caption{\textbf{OOB error vs number of trees} Density contours are also shown.}
\end{figure}

Thus, the optimal number of trees is around 170-200, where the OOB error stabilizes, and going further only makes the model more computationally expensive.

Re-running the model for the optimal tree number, the test-classification error now is 0.04.


(e) Calculate the feature importance. In your report, describe and interpret the feature importances. Retrain the model using a subset of the most important features. In your report, indicate which features you have chosen and compare the retrained classifier with the original classifier.


\subsection*{(e)}

One characteristic of the random forest classifier is it can give us the relative importance of features. It is computed as how much of a reduction in the Gini index (or other used criterion) is brought by that feature. Were it not for the bootstrapping and subset of features to split from, there would not be enough information for that measure to be computed accurately\cite[pp. 345-346]{james2013introduction}. In this case, we obtained the following plot, with the 50 most important features:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{../Plots/B_Q4e_1.png}
    \caption{\textbf{Random Forest Feature Importance}, for the 50 most important features}
\end{figure}

We see that feature importance decreases very rapidly before steadily decreasing slowly. Taking the first 20, another random forest model can be trained. With this model, a test set classification error of 0.0999 was obtained. As expected, the performance is slightly lower but considering the amount by which computational complexity was decreased, this is a very positive result.


\subsection*{(f)}

Here, the previous questions are reproduced using another supervised learning method. The chosen method is Support Vector Machines (SVM), as it is quite good when the data is high dimensional, and even when there are more dimensions than samples \cite{svm_sklearn}. However, in order to get the feature importances back from the SVM model, one needs to use a linear kernel. 

The data was preprocessed in this same way as for the Random Forest Classifier. Duplicates, low-variance, and higly correlated rows and features are dealt with, and outliers are left alone since the SVM algorithm is robust to them.  

Running this, a test set classification error of 0.0899 was obtained. And using the \texttt{coeff\_} output of the model, the feature importances were obtained and plotted in figure 7. As one cans see there is some agreement between the 2 models, with the most important feature being the same and a few features with relatively similar importance (Fea36, Fea354,\ldots)  
Training the SVM model on just the most important feature, the test set classification error roughly doubles but stays indicative of good performance.

\begin{figure}{h}
    \centering
    \includegraphics[width=0.5\textwidth]{../Plots/B_Q4e_2.png}
    \caption{\textbf{SVM Feature Importance}, for the 50 most important features}
\end{figure}

\newpage

\section*{Question 5}

\subsection*{(a)}

Two different clustering techniques are explored in this question. One of them will be K-means which has been already discussed partly. It will place a desired number of initial centroids in the feature space and update their position based on cluster members until convergence. The two main outputs are then the cluster centroids' final location and the cluster memberships of all the datasets' points.  
The second technique will Gaussian Mixture Models. The idea behind this is to assume the entire data is distributed as the sum of multiple Gaussian distributions with unknown parameters\cite[p. 260]{sklearn_book}. 




\bibliographystyle{plain}
\bibliography{refs.bib}


\end{document}